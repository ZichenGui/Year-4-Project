\chapter{Proof of theorem \ref{thm frequency dist}} \label{proof of thm frequency dist}
\textit{Proof:} To prove the theorem, we need to use Central Limit Theorem. The variant used in the proof is perceived by Russian mathematician Aleksandr Lyapunov where the random variables only need to be independent, not necessarily identical.


\begin{theorem}[Central Limit Theorem (Lyapunov)] \label{CLT}
Let $\{X_1, \cdots, X_n\}$ be a sequence of $n$ independent distributed random variables with $E(X_i) = \mu_i$ and $Var(X_i) = \sigma_i^2$. Define $s_n^2 = \sum_{i = 1}^{n} \sigma_i^2$. If for some $\delta > 0$, Lyapunov's condition \\
\begin{align*}
	\lim_{n \rightarrow \infty} \frac{1}{s_{n}^{2+\delta}} \sum_{i=1}^{n} E\left[\:\abs{X_i - \mu_i}^{2+\delta}\right] = 0
\end{align*}
is satisfied, then as $n$ approaches infinity,
\begin{align*}
	\frac{1}{s_n} \sum_{i=1}^{n} (X_i - \mu_i) \xrightarrow[]{d} \mathcal{N}(0, 1).
\end{align*}
\end{theorem}


Our proof will take three steps. In the first step, we will compute $s_{n(p+1)}^2(j)$. After that, we will argue that the Lyapunov's condition is indeed satisfied. Finally, we will apply CLT and derive equation \ref{frequency dist}.

\textbf{Step 1:} We compute $s_{n(p+1)}^2(j)$ defined in the theorem as:
\begin{align}
	s_{n(p+1)}^2(j) & = \sum_{i=1}^{n(p+1)} Var(X_i^j) \label{thm1 1.1} \\
		 		 	& = \sum_{i=1}^{n(p+1)} \Pi_{index(i)}(j) \cdot (1 - \Pi_{index(i)}(j)) \label{thm1 1.2} \\
		  		 	& = \sum_{l = 1}^{n} \sum_{i = 0}^{p} \Pi_i(j) (1 - \Pi_i(j))  \label{thm1 1.3}\\
		  		 	& = n \cdot \sum_{i = 0}^{p} \Pi_i(j) (1 - \Pi_i(j)) \label{thm1 1.4}.
\end{align}


The computation is straightforward. In equation \ref{thm1 1.1}, we use the definition of $s_{n(p+1)}^2(j)$ in theorem \ref{CLT}. For Bernoulli distribution with success probability $p$, the variance is $p \cdot (1 - p)$, hence equation \ref{thm1 1.2}. But by construction, we sample from $\Pi_i(j)$ for $i \in \{0, \cdots, p\}$ $n$ times each (In this case, we treat the actual input as a random variable too.), so we can re-write the equation as \ref{thm1 1.3}. Finally, the inner summation is not dependent on $l$, so the double summation is nothing but $n$ times of the inner summation in \ref{thm1 1.4}.


\textbf{Step 2:} As $X_i^j$'s are Bernoulli random variables, all moments exist, whence Lyapunovs condition is satisfied for any $\delta > 0$. So we can apply CLT to conclude that as n goes to infinity,
\begin{align}
	\frac{1}{s_{n(p+1)}(j)} \sum_{i=1}^{n(p+1)} (X_i^j - \mu_i) \xrightarrow[]{d} \mathcal{N}(0, 1). \label{thm1 2.1}
\end{align}


\textbf{Step 3:} In reality, the observed variable is $\sum_{i} X_i^j$, or plainly, the number of occurrences of attribute $j$. We can obtain the distribution of $\sum_{i} X_i^j$ from equation \ref{thm1 2.1} as:
\begin{align*}
	\sum_{i=1}^{n(p+1)} (X_i^j - \mu_i) & \xrightarrow[]{d} \mathcal{N}\left(0, s_{n(p+1)}^2(j)\right) \\
	\sum_{i=1}^{n(p+1)} X_i^j - \sum_{i=1}^{n(p+1)} \mu_i & \xrightarrow[]{d} \mathcal{N}\left(0, s_{n(p+1)}^2(j)\right) \\
	\sum_{i=1}^{n(p+1)} X_i^j - \sum_{l = 1}^{n} \sum_{i=0}^{p} \mu_i & \xrightarrow[]{d} \mathcal{N}\left(0, s_{n(p+1)}^2(j)\right) \\
	\sum_{i=1}^{n(p+1)} X_i^j - \sum_{l = 1}^{n} \sum_{i=0}^{p} \Pi_i(j) & \xrightarrow[]{d} \mathcal{N}\left(0, s_{n(p+1)}^2(j)\right) \\
	\sum_{i=1}^{n(p+1)} X_i^j - n \cdot \frac{p+1}{k} & \xrightarrow[]{d} \mathcal{N}\left(0, s_{n(p+1)}^2(j)\right) \\
	\sum_{i=1}^{n(p+1)} X_i^j & \xrightarrow[]{d} \mathcal{N}\left(\frac{n \cdot (p+1)}{k}, s_{n(p+1)}^2(j)\right) \numberthis \label{thm1 3.1} \qquad \square
\end{align*}